# -*- coding: utf-8 -*-
"""SegFormer_whole_lung_seg_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvXQGjDYJVPkvsZS9vmuG_RdAWRpRk7U

## Colab í™˜ê²½ì„¤ì •
"""

# ==================================================
# Google Drive ë§ˆìš´íŠ¸
# ==================================================

from google.colab import drive
drive.mount('/content/drive')

# ==================================================
# Google Drive ë°ì´í„° í™•ì¸ ì˜¤ë¥˜ í•´ê²°
# ==================================================

# # "image file is truncated" ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ëª¨ë‘ ë¶ˆëŸ¬ì˜¤ê¸°
# ImageFile.LOAD_TRUNCATED_IMAGES = True

# ==================================================
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (colab ê¸°ì¤€)
# ==================================================

!pip install transformers accelerate torchvision
!pip install optuna

"""## í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"""

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import copy
import zipfile
import pickle
import random
import pytz
from glob import glob
from datetime import datetime, timedelta, timezone
from collections import Counter
from multiprocessing import Pool
from os.path import basename, splitext

# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# ë¨¸ì‹ ëŸ¬ë‹ & í‰ê°€ ì§€í‘œ
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    jaccard_score,
    precision_score,
    recall_score,
)

# PyTorch & TorchVision
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import torch.nn as nn
import torch.nn.functional as F

# HuggingFace Transformers
from transformers import (
    SegformerConfig,
    SegformerForSemanticSegmentation,
    SegformerImageProcessor,
)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
import optuna

# Colab ì „ìš©
from google.colab import drive

"""## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì†ì„± íŒŒì•…í•˜ê¸°"""

# ==================================================
# ê²½ë¡œ ì„¤ì • ë° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
# ==================================================

# íŒŒì¼ ê²½ë¡œ ì§€ì •
DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/projects/kdt_med_seg/raw_data/chest_x_ray'

paths = {
    'train_X': os.path.join(DATA_DIR, 'X_train.npy'),
    'train_y': os.path.join(DATA_DIR, 'y_train.npy'),
    'val_X'  : os.path.join(DATA_DIR, 'X_val.npy'),
    'val_y'  : os.path.join(DATA_DIR, 'y_val.npy'),
    'test_X' : os.path.join(DATA_DIR, 'X_test.npy'),
    'test_y' : os.path.join(DATA_DIR, 'y_test.npy'),
}

# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
print("\n\n" + "=" * 50)
print("ğŸ” .npy Dataset Path & File Check")
print("=" * 50)

for name, path in paths.items():
    exists = os.path.exists(path)
    print(f"{name:10s} â†’ {path} | Exists: {exists}")

# .npy íŒŒì¼ ë¡œë”©
train_images = np.load(paths['train_X'])
train_masks  = np.load(paths['train_y'])

val_images = np.load(paths['val_X'])
val_masks  = np.load(paths['val_y'])

test_images = np.load(paths['test_X'])
test_masks  = np.load(paths['test_y'])

# ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥
print("\n\n" + "=" * 50)
print("ğŸ” Dataset Directory & File Count Check")
print("=" * 50)

print(f"Train ğŸ‘‰ Images: {train_images.shape}, Masks: {train_masks.shape}")
print(f"Valid ğŸ‘‰ Images: {val_images.shape}, Masks: {val_masks.shape}")
print(f"Test  ğŸ‘‰ Images: {test_images.shape}, Masks: {test_masks.shape}")

# ==================================================
# ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ìŒì˜ íŒŒì¼ëª…, í¬ê¸° ë° ì±„ë„ ì •í•©ì„± í™•ì¸
# ==================================================

def check_array_pair_consistency(images: np.ndarray,
                                 masks: np.ndarray,
                                 set_name: str = "Set") -> None:

    print("\n" + "=" * 50)
    print(f"ğŸ” Consistency Check for {set_name}")
    print("=" * 50)

    # Sample count
    if len(images) != len(masks):
        print(f"[â—] Sample count mismatch: {len(images)} images vs {len(masks)} masks")
    else:
        print(f"[âœ…] Sample count: {len(images)} (matched)")

    # Shape mismatch detection
    shape_mismatches = []
    for i, (img, msk) in enumerate(zip(images, masks)):
        if img.shape != msk.shape:
            shape_mismatches.append((i, img.shape, msk.shape))

    if shape_mismatches:
        print(f"[â—] Found {len(shape_mismatches)} shape mismatches:")
        for idx, img_shape, mask_shape in shape_mismatches[:5]:
            print(f"   - Index {idx}: image={img_shape}, mask={mask_shape}")
        if len(shape_mismatches) > 5:
            print("   ...")
    else:
        print("[âœ…] All image/mask shapes match.")

    # Channel count summary (last dim)
    def get_channel_count(arr):
        return arr.shape[-1] if arr.ndim == 4 else 1

    img_chan = get_channel_count(images)
    mask_chan = get_channel_count(masks)

    if img_chan != mask_chan:
        print(f"[â—] Channel count mismatch: image={img_chan}, mask={mask_chan}")
    else:
        print(f"[âœ…] Channel count: {img_chan} (matched)")

# Run consistency checks
check_array_pair_consistency(train_images, train_masks, "Train Set")
check_array_pair_consistency(val_images, val_masks, "Validation Set")
check_array_pair_consistency(test_images, test_masks, "Test Set")

# ==================================================
# ì‚¬ì „ ì§„ë‹¨: 255 í¬í•¨ ë§ˆìŠ¤í¬ ìˆ˜ í™•ì¸
# ==================================================

def count_masks_with_255(masks: np.ndarray, set_name: str = "Set"):

    print("\n" + "=" * 50)
    print(f"ğŸ” Checking 255 presence in {set_name}")
    print("=" * 50)

    # Remove last dimension if it's singleton (e.g., shape: (N, H, W, 1))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    # Vectorized check: (mask == 255).any over (H, W)
    count = int((masks == 255).any(axis=(1, 2)).sum())
    total = len(masks)

    print(f"âš ï¸  Masks with 255: {count} / {total}")

# Run the check for all sets
count_masks_with_255(train_masks, "Train Set")
count_masks_with_255(val_masks,   "Validation Set")
count_masks_with_255(test_masks,  "Test Set")

# ==================================================
# ë§ˆìŠ¤í¬ í”½ì…€ê°’ ë¶„í¬ í™•ì¸í•˜ê¸°
# ==================================================

def analyze_mask_pixel_distribution_np(masks: np.ndarray,
                                       set_name: str = "Set"):

    print("\n" + "="*50)
    print(f"ğŸ” Analysing pixel-value distribution â€” {set_name}")
    print("="*50)

    # Squeeze last dim if singleton (e.g., (N, H, W, 1) â†’ (N, H, W))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    unique_sets = [tuple(sorted(np.unique(m))) for m in masks]
    value_counter = Counter(unique_sets)

    all_binary = all(len(u) == 2 for u in unique_sets)
    common_set = unique_sets[0]
    all_same = all_binary and all(u == common_set for u in unique_sets)

    # print summary
    print(f"Total masks analysed        : {len(masks)}")
    print(f"Are all masks binary (2 vals)?  {'Yes' if all_binary else 'No'}")
    print(f"Do all masks share same vals?   {'Yes' if all_same else 'No'}")

    print("\nğŸ“Š Pixel-value combinations across masks:")
    for vals, cnt in value_counter.items():
        print(f"  {vals} â†’ {cnt} mask(s)")

    return {
        "all_binary": all_binary,
        "all_same": all_same,
        "value_counter": value_counter
    }

# Run on each split
train_summary = analyze_mask_pixel_distribution_np(train_masks, "Train Set")
val_summary   = analyze_mask_pixel_distribution_np(val_masks,   "Validation Set")
test_summary  = analyze_mask_pixel_distribution_np(test_masks,  "Test Set")

# ==================================================
# ì´ë¯¸ì§€ í¬ê¸° í†µê³„ ìˆ˜ì§‘
# ==================================================

def get_dims_from_arrays(arr: np.ndarray):
    if arr.ndim == 4:       # (N, H, W, C)
        _, H, W, _ = arr.shape
    elif arr.ndim == 3:     # (N, H, W)
        _, H, W = arr.shape
    else:
        raise ValueError("Array must be 3-D or 4-D (batch of images).")

    # All samples share the same H and W here, but we still create per-sample arrays
    # in case your future dataset mixes sizes.
    widths  = np.full(len(arr), W)
    heights = np.full(len(arr), H)
    return widths, heights

# Printer
def print_size_stats(name: str, widths: np.ndarray, heights: np.ndarray):
    ratios = widths / heights
    print("\n" + "=" * 60)
    print(f"ğŸ–¼ï¸  {name} â€” Size Statistics")
    print("=" * 60)
    print(f"Samples               : {len(widths)}")
    print(f"Width   â†’ min {widths.min()},  max {widths.max()},  "
          f"mean {widths.mean():.2f},  median {np.median(widths)}")
    print(f"Height  â†’ min {heights.min()}, max {heights.max()}, "
          f"mean {heights.mean():.2f}, median {np.median(heights)}")
    print(f"Ratio W/H â†’ min {ratios.min():.3f}, max {ratios.max():.3f}, "
          f"mean {ratios.mean():.3f}, median {np.median(ratios):.3f}")

# Analyse any split you like
def analyse_split(images: np.ndarray, masks: np.ndarray, split: str):
    W_img, H_img = get_dims_from_arrays(images)
    W_msk, H_msk = get_dims_from_arrays(masks)

    print_size_stats(f"Images â€” {split}", W_img, H_img)
    print_size_stats(f"Masks  â€” {split}", W_msk, H_msk)

# Run on each set
analyse_split(train_images, train_masks, "Train")
analyse_split(val_images,   val_masks,   "Validation")
analyse_split(test_images,  test_masks,  "Test")

"""## í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í• """

# ==================================================
# ë¶„í• ëœ ë°ì´í„° ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¨ ë’¤ shape í™•ì¸
# ==================================================

DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/projects/kdt_med_seg/raw_data/chest_x_ray'

paths = {
    'train_X': os.path.join(DATA_DIR, 'X_train.npy'),
    'train_y': os.path.join(DATA_DIR, 'y_train.npy'),
    'val_X'  : os.path.join(DATA_DIR, 'X_val.npy'),
    'val_y'  : os.path.join(DATA_DIR, 'y_val.npy'),
    'test_X' : os.path.join(DATA_DIR, 'X_test.npy'),
    'test_y' : os.path.join(DATA_DIR, 'y_test.npy'),
}

# ==================================================
# 2. .npy íŒŒì¼ ë¡œë”©
# ==================================================
X_train = np.load(paths['train_X'], allow_pickle=True)
y_train = np.load(paths['train_y'], allow_pickle=True)
X_val   = np.load(paths['val_X'],   allow_pickle=True)
y_val   = np.load(paths['val_y'],   allow_pickle=True)
X_test  = np.load(paths['test_X'],  allow_pickle=True)
y_test  = np.load(paths['test_y'],  allow_pickle=True)

print("\nğŸ“Š Loaded Dataset Shapes:")
print(f"Train: X = {X_train.shape}, y = {y_train.shape}")
print(f"Val  : X = {X_val.shape},   y = {y_val.shape}")
print(f"Test : X = {X_test.shape},  y = {y_test.shape}")

"""## HuggingFace SegFormer ì „ì²˜ë¦¬ê¸° ì„¤ì •


"""

# ==================================================
# X_train imageì˜ RGB ê¸°ì¤€ í”½ì…€ê°’ì˜ mean/std ì‚°ì¶œ
# ==================================================

def compute_rgb_mean_std_from_npy(images: np.ndarray):

    # dtypeÂ·ìŠ¤ì¼€ì¼ ì •ê·œí™”
    if images.dtype == np.uint8:
        images = images.astype(np.float32) / 255.0  # 0~1 ë³€í™˜
    else:
        images = images.astype(np.float32)

    # ì±„ë„Â·shape ì •ë¦¬
    if images.ndim == 4 and images.shape[-1] == 1:         # (N, H, W, 1) â†’ í‘ë°±
        images = images.squeeze(-1)                        # (N, H, W)
    if images.ndim == 3:                                   # (N, H, W) â† í‘ë°±
        # í‘ë°± â†’ RGB 3ì±„ë„ ë³µì œ (SegFormer ì…ë ¥ í˜¸í™˜)
        images = np.repeat(images[:, :, :, None], 3, axis=-1)  # (N, H, W, 3)

    # 2) ì±„ë„ë³„ mean/std ê³„ì‚° (ë²¡í„°í™”)
    mean = images.mean(axis=(0, 1, 2))   # shape (3,)
    std  = images.std(axis=(0, 1, 2))    # shape (3,)

    # 3) ì¶œë ¥
    mean_list = mean.tolist()
    std_list  = std.tolist()
    print(f"ğŸ“Œ Train mean : {mean_list}")
    print(f"ğŸ“Œ Train std  : {std_list}")
    return mean_list, std_list

# ì‚¬ìš© ì˜ˆì‹œ
train_mean, train_std = compute_rgb_mean_std_from_npy(X_train)

# ==================================================
# ì „ì²˜ë¦¬ê¸°ì— ì ìš©í•  config ëª¨ìŒ
# ==================================================

# ê°œë³„ config ì„ ì–¸
# resize = Falseì¸ ê²½ìš° sizeëŠ” ë¬´ì‹œ
SegFormer_config_no_resize = {
    "name": "SegFormer-b0-no-resize-128",
    "model_name": "nvidia/segformer-b0",
    "resize": False,
    "size": {"height": 128, "width": 128},
    "normalize": True,
    "reduce_labels": False,
    "image_mean": train_mean,
    "image_std": train_std,
    "rescale": False
}

print("\nğŸ“Š Image Stats:", SegFormer_config_no_resize["name"])
print("Image mean:", SegFormer_config_no_resize["image_mean"])
print("Image std:", SegFormer_config_no_resize["image_std"])

# ì„ ì–¸ëœ Config ì„¤ì • ëª¨ìŒì§‘
experiment_configs = {
    "no_resize_128": SegFormer_config_no_resize
}

# ==================================================
# ì‚¬ìš©í•  Config ì„ íƒ
# ==================================================

# ì„¤ì • ëª¨ìŒì§‘ ì¤‘ ì‚¬ìš©í•  config ì„ íƒ
current = experiment_configs["no_resize_128"]

# ì„ íƒëœ ì„¤ì •ì„ SegformerImageProcessorì— ì ìš©
image_processor = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

print("\nğŸ“Š Preprocessing Stats: image_processor")
print("Image mean:", image_processor.image_mean)
print("Image std:", image_processor.image_std)

"""## HuggingFace SegFormer Dataset ê°ì²´ ì •ì˜ ë° Dataset êµ¬ì„±"""

# ==================================================
# Dataset ê°ì²´ ì •ì˜ (HuggingFace SegFormer Pretrained)
# ==================================================

# Dataset ê°ì²´ ì •ì˜
class HFSegformerDataset(Dataset):
    def __init__(self, images, masks, processor, debug=False):
        """
        images: np.ndarray  (N, H, W)  ë˜ëŠ” (N, H, W, 1)
        masks : np.ndarray  (N, H, W)  ë˜ëŠ” (N, H, W, 1)
        """
        self.images = images
        self.masks  = masks
        self.processor = processor
        self.debug = debug
        if self.debug:
            self.mask_stats = []

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        import torch                       # ë¡œì»¬ import (NameError ë°©ì§€)

        # ì´ë¯¸ì§€Â·ë§ˆìŠ¤í¬ ì¶”ì¶œ
        img = self.images[idx].squeeze(-1)         # (H, W)  float32 (0~1)
        msk = self.masks[idx].squeeze(-1).astype(np.int64)  # (H, W) 0/1

        # í‘ë°± â†’ RGB 3ì±„ë„ ë³µì œ
        img_rgb = np.repeat(img[..., None], 3, axis=-1)     # (H,W,3)

        # HuggingFace processor ì ìš©
        inputs = self.processor(
            images            = img_rgb,
            segmentation_maps = msk,
            return_tensors    = "pt"
        )

        # labels ê°’ì´ processor ë‚´ë¶€ì—ì„œ int64 íƒ€ì…ìœ¼ë¡œ ë°”ë€Œë¯€ë¡œ Clamp ìƒëµ ê°€ëŠ¥
        # í•„ìš”í•˜ë‹¤ë©´ .clamp(0, 1) ìœ ì§€
        inputs["labels"] = inputs["labels"].clamp(0, 1)

        # squeeze(0) í•´ì„œ (3,H,W) / (H,W) ë°˜í™˜
        out = {k: v.squeeze(0) for k, v in inputs.items()}

        # ë””ë²„ê¹…ìš© í†µê³„
        if self.debug:
            self.mask_stats.append(torch.unique(out["labels"]).tolist())

        return out

    # ì—í¬í¬ ëë‚˜ê³  í˜¸ì¶œ
    def report_mask_stats(self):
        if self.debug and self.mask_stats:
            uniq = sorted({val for sub in self.mask_stats for val in sub})
            print(f"[Epoch Summary] Unique label values: {uniq}")
            self.mask_stats.clear()

# ==================================================
# ë°ì´í„°ì…‹ êµ¬ì„± í•¨ìˆ˜
# ==================================================

# ë°ì´í„°ì…‹ êµ¬ì„± -> í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì—ì„œ í˜¸ì¶œ
def build_covid_dataloaders(X_train, y_train,
                             X_val, y_val,
                             X_test, y_test,
                             processor,
                             batch_size=8,
                             debug=False,
                             num_workers=4):

    # Dataset ê°ì²´ ìƒì„±
    train_dataset = HFSegformerDataset(X_train, y_train, processor, debug=debug)
    val_dataset   = HFSegformerDataset(X_val,   y_val,   processor, debug=debug)
    test_dataset  = HFSegformerDataset(X_test,  y_test,  processor, debug=debug)

    # DataLoader êµ¬ì„±
    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                              shuffle=True, num_workers=num_workers)

    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                            shuffle=False, num_workers=num_workers)

    test_loader = DataLoader(test_dataset, batch_size=batch_size,
                             shuffle=False, num_workers=num_workers)

    return train_loader, val_loader, test_loader

"""## HuggingFace SegFormer ëª¨ë¸ ìƒì„± ë° í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ (ë°ì´í„°ì…‹ êµ¬ì„± ë° ëª¨ë¸ì €ì¥ í¬í•¨)"""

# ==================================================
# ì—…ìƒ˜í”Œë§ì„ ìœ„í•œ í´ë˜ìŠ¤
# ==================================================

class SegFormerWithUpsample(nn.Module):
    def __init__(self, base_model, upsample_size=(128, 128)):
        super().__init__()
        self.base_model = base_model
        self.upsample = nn.Upsample(size=upsample_size, mode="bilinear", align_corners=False)

    def forward(self, pixel_values, labels=None):
        outputs = self.base_model(pixel_values=pixel_values, labels=labels)
        logits = outputs.logits                    # (B, C, H_out, W_out)
        logits_up = self.upsample(logits)          # (B, C, H, W)

        # ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•´ GTì™€ í¬ê¸° ì¼ì¹˜
        loss = None
        if labels is not None:
            loss = F.cross_entropy(logits_up, labels, ignore_index=255)

        return type('Output', (), {
            'logits': logits_up,
            'loss': loss
        })()

# ==================================================
# Segformer-B0 ê¸°ë°˜ scratch ëª¨ë¸ ìƒì„±
# ==================================================

# ì „ì²˜ë¦¬ê¸° current ì„¤ì •(dict)ì„ ê¸°ë°˜ìœ¼ë¡œ SegformerConfigë¥¼ ìƒì„±
def create_config_from_preprocessor(
    current,
    num_labels=2,
    hidden_sizes=[32, 64, 160, 256],
    decoder_hidden_size=256,
    dropout=0.1,
    ignore_index=255
):
    config = SegformerConfig(
        num_labels=num_labels,
        image_size=(current["size"]["height"], current["size"]["width"]),
        do_reduce_labels=current.get("reduce_labels", False),
        hidden_sizes=hidden_sizes,
        decoder_hidden_size=decoder_hidden_size,
        classifier_dropout_prob=dropout,
        semantic_loss_ignore_index=ignore_index
    )
    return config

# current ë³€ìˆ˜ì— ì €ì¥ëœ ì „ì²˜ë¦¬ê¸° configë¥¼ ë°˜ì˜í•˜ì—¬ ëª¨ë¸ ìƒì„±
current = experiment_configs["no_resize_128"]
config = create_config_from_preprocessor(current, num_labels=2)
base_model = SegformerForSemanticSegmentation(config)
model = SegFormerWithUpsample(base_model, upsample_size=(128, 128))

# ==================================================
# í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì— í•„ìš”í•œ ë³´ì¡° í•¨ìˆ˜ ì •ì˜
# ==================================================

# í•˜ë‚˜ì˜ epoch ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµ -> í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì—ì„œ í˜¸ì¶œ
def train_one_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0.0
    total_dice = 0.0

    progress_bar = tqdm(dataloader, desc="ğŸŸ¢ Training", leave=True, dynamic_ncols=True, mininterval=0.1)

    for batch in progress_bar:
        inputs = batch["pixel_values"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(pixel_values=inputs, labels=labels)
        loss = outputs.loss

        with torch.no_grad():
            preds = torch.argmax(outputs.logits, dim=1)
            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()
            dice = compute_dice_score(preds, labels)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        total_dice += dice.item()

    progress_bar.set_description("ğŸŸ¢ Training (completed)")
    return total_loss / len(dataloader), total_dice / len(dataloader)

# ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì„ í‰ê°€ -> í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì—ì„œ í˜¸ì¶œ
def validate(model, dataloader, device):
    model.eval()
    total_loss = 0.0
    total_dice = 0.0
    total_iou = 0.0
    total_precision = 0.0
    total_recall = 0.0
    n = 0

    progress_bar = tqdm(dataloader, desc="ğŸ”µ Validating", leave=True, dynamic_ncols=True, mininterval=0.1)

    with torch.no_grad():
        for batch in progress_bar:
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs, labels=labels)
            loss = outputs.loss
            preds = torch.argmax(outputs.logits, dim=1)

            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()
            dice = compute_dice_score(preds, labels)

            y_pred = preds.cpu().numpy().flatten()
            y_true = labels.cpu().numpy().flatten()

            iou = jaccard_score(y_true, y_pred, average="binary", zero_division=0)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)

            total_loss += loss.item()
            total_dice += dice.item()
            total_iou += iou
            total_precision += precision
            total_recall += recall
            n += 1

    progress_bar.set_description("ğŸ”µ Validating (completed)")
    return {
        "loss": total_loss / n,
        "dice": total_dice / n,
        "iou": total_iou / n,
        "precision": total_precision / n,
        "recall": total_recall / n
    }

# ì´ì§„ Dice Score ê³„ì‚° -> í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì—ì„œ í˜¸ì¶œ
def compute_dice_score(preds, targets, smooth=1e-6):
    preds = preds.view(-1).float()
    targets = targets.view(-1).float()

    intersection = (preds * targets).sum()
    dice = (2.0 * intersection + smooth) / (preds.sum() + targets.sum() + smooth)

    return dice

# ==================================================
# í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ ì €ì¥
# ==================================================

# í•™ìŠµ ì •ìƒ ì™„ë£Œ í›„ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ëª¨ë¸ì €ì¥ -> í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ì—ì„œ í˜¸ì¶œ
def save_segformer_model_simple(
    model,
    processor=None,
    history=None,
    config=None,
    save_dir=None,
    tag=None
):
    """
    SegFormer-B0 ëª¨ë¸ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê°„ë‹¨íˆ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        model: í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸
        processor: SegformerImageProcessor ê°ì²´ (ì„ íƒ)
        history: í•™ìŠµ ê¸°ë¡ (loss/dice ë“± í¬í•¨ëœ dict, ì„ íƒ)
        config: í•™ìŠµ ì„¤ì • ì •ë³´ (ì˜ˆ: batch_size, lr ë“±, ì„ íƒ)
        save_dir: ì €ì¥ ë””ë ‰í† ë¦¬ëª… ì ‘ë‘ì‚¬
        tag: ì „ì²˜ë¦¬ê¸° ì„¤ì •ëª… ë“± (ex. "no_resize")

    Returns:
        str: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    current_tag = tag or "unknown"
    save_dir = save_dir or f"{current_tag}"

    KST = timezone(timedelta(hours=9))
    timestamp = datetime.now(KST).strftime("%m%d_%H%M")
    model_path = f"/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/{save_dir}_{timestamp}"
    os.makedirs(model_path, exist_ok=True)

    # ëª¨ë¸ ì €ì¥ (HuggingFace í¬ë§·)
    model.save_pretrained(model_path)
    if processor:
        processor.save_pretrained(model_path)

    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
    def safe_get(metric_name):
        return history[metric_name][-1] if history and metric_name in history else None

    metadata = {
        "timestamp": timestamp,
        "arch": "SegFormer-B0",
        "num_labels": model.config.num_labels,
        "input_size": processor.size if processor and hasattr(processor, "size") else None,
        "label_mode": "binary",
        "mask_mode": "L",
        "is_scratch": True,
        "config": config,
        "metrics": {
            "val_loss": safe_get("val_loss"),
            "val_dice": safe_get("val_dice"),
            "val_iou": safe_get("val_iou"),
            "val_precision": safe_get("val_precision"),
            "val_recall": safe_get("val_recall")
        }
    }

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_filename = f"metadata_{current_tag}.pt"
    torch.save(metadata, os.path.join(model_path, metadata_filename))

    # best weight ê°™ì´ ì €ì¥ (state_dict í˜•ì‹)
    torch.save(model.state_dict(), os.path.join(model_path, "best_state_dict.pt"))

    print(f"\nâœ… Model saved to: {model_path}")
    if processor:
        print("ğŸ§ª Processor saved")
    print(f"ğŸ§¾ Metadata saved as {metadata_filename}")
    return model_path

# ==================================================
# í•™ìŠµ ìˆ˜í–‰ í•¨ìˆ˜ ì •ì˜
# ==================================================

# ì „ì²˜ë¦¬ê¸° ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•œ í•™ìŠµ ëª¨ë¸ ì‚¬ìš©
# train_one_epoch, validate, compute_dice_score  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰

def train_segformer_model(
    model,
    image_processor,
    train_loader,
    val_loader,
    batch_size=4,
    lr=5e-5,
    num_epochs=20,
    weight_decay=1e-4,
    early_stopping_metric="dice",  # "loss" ë˜ëŠ” "dice"
    patience=5,
    tag=None
):
    """
    SegFormer ëª¨ë¸ í•™ìŠµ ë£¨í”„ ìˆ˜í–‰ + EarlyStopping + ëª¨ë¸ ì €ì¥ í¬í•¨

    Args:
        model: HuggingFace SegformerForSemanticSegmentation ëª¨ë¸
        image_processor: SegformerImageProcessor
        train_loader: í•™ìŠµìš© DataLoader
        val_loader: ê²€ì¦ìš© DataLoader
        batch_size: í•™ìŠµì— ì‚¬ìš©í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        lr: í•™ìŠµë¥  (learning rate)
        num_epochs: ìµœëŒ€ ì—í­ ìˆ˜
        weight_decay: AdamW ì˜µí‹°ë§ˆì´ì € weight decay
        early_stopping_metric: "dice" or "loss"
        patience: early stopping í—ˆìš© ì—í­ ìˆ˜
        tag: ëª¨ë¸ ì €ì¥ ì´ë¦„ ì ‘ë‘ì–´ (ì˜ˆ: "no_resize")
    """

    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    history = {
        "train_loss": [],
        "val_loss": [],
        "train_dice": [],
        "val_dice": [],
        "val_iou": [],
        "val_precision": [],
        "val_recall": []
    }

    best_metric = -np.inf if early_stopping_metric == "dice" else np.inf
    no_improve_epochs = 0

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    if device.type == "cuda":
        print(f"ğŸš€ Training started on device: {device} ({torch.cuda.get_device_name(device)})")
    else:
        print(f"ğŸš€ Training started on device: {device} (CPU only)")

    for epoch in range(1, num_epochs + 1):
        print(f"\n===== Epoch {epoch}/{num_epochs} =====")

        # ğŸ” Train
        avg_train_loss, avg_train_dice = train_one_epoch(model, train_loader, optimizer, device)

        # ğŸ” Validate
        val_metrics = validate(model, val_loader, device)

        # âœ… ê¸°ë¡
        history["train_loss"].append(avg_train_loss)
        history["train_dice"].append(avg_train_dice)
        history["val_loss"].append(val_metrics["loss"])
        history["val_dice"].append(val_metrics["dice"])
        history["val_iou"].append(val_metrics["iou"])
        history["val_precision"].append(val_metrics["precision"])
        history["val_recall"].append(val_metrics["recall"])

        print(f"ğŸ“Š Train Loss: {avg_train_loss:.4f}, Dice: {avg_train_dice:.4f}")
        print(f"ğŸ§ª Val   Loss: {val_metrics['loss']:.4f}, Dice: {val_metrics['dice']:.4f}")

        # â³ Early stopping check
        current_metric = val_metrics["dice"] if early_stopping_metric == "dice" else val_metrics["loss"]
        is_improved = (current_metric > best_metric) if early_stopping_metric == "dice" else (current_metric < best_metric)

        if is_improved:
            best_metric = current_metric
            no_improve_epochs = 0
            print(f"âœ… Improved {early_stopping_metric}! Model saved.")
        else:
            no_improve_epochs += 1
            print(f"âš ï¸ No improvement in {early_stopping_metric} ({no_improve_epochs}/{patience})")

            if no_improve_epochs >= patience:
                print(f"ğŸ›‘ Early stopping triggered. Best {early_stopping_metric}: {best_metric:.4f}")
                break

    print("\nğŸ Training complete.")

    # ğŸ’¾ í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
    save_segformer_model_simple(
        model=model,
        processor=image_processor,
        history=history,
        config={
            "batch_size": batch_size,
            "lr": lr,
            "num_epochs": num_epochs,
            "weight_decay": weight_decay,
            "early_stopping_metric": early_stopping_metric,
            "patience": patience
        },
        tag=tag or "segformer"
    )

    return model, history

"""## HuggingFace SegFormer ëª¨ë¸í‰ê°€, ì„±ëŠ¥ì§€í‘œ ë° ì‹œê°í™” ìë£Œ ì¶œë ¥ í•¨ìˆ˜"""

# ==================================================
# í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ ì •ì˜
# ==================================================

def evaluate_on_testset(
    model,
    test_loader,          # â† DataLoader ì§ì ‘ ì „ë‹¬
    device=None,
    tag=None,
    return_y=False
):

    device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    total_loss = 0.0
    all_preds, all_labels = [], []

    progress = tqdm(test_loader, desc="ğŸ§ª Evaluating on Test Set",
                    leave=True, dynamic_ncols=True, mininterval=0.1)

    with torch.no_grad():
        for batch in progress:
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs, labels=labels)
            total_loss += outputs.loss.item()

            preds = torch.argmax(outputs.logits, dim=1)

            all_preds.append(preds.cpu().numpy().flatten())
            all_labels.append(labels.cpu().numpy().flatten())

    progress.set_description("ğŸ§ª Evaluating (completed)")

    y_pred = np.concatenate(all_preds)
    y_true = np.concatenate(all_labels)

    # ì§€í‘œ ê³„ì‚°
    dice      = compute_dice_score(torch.tensor(y_pred),
                                   torch.tensor(y_true)).item()
    iou       = jaccard_score(y_true, y_pred, average="binary")
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall    = recall_score(y_true, y_pred, zero_division=0)
    accuracy  = accuracy_score(y_true, y_pred)
    avg_loss  = total_loss / len(test_loader)

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()
    specificity = tn / (tn + fp + 1e-8)

    print(f"\nâœ… [Test Set Evaluation Complete: {tag}]")
    print(f"ğŸ“Š Loss     : {avg_loss:.4f}")
    print(f"ğŸ¯ Dice     : {dice:.4f}")
    print(f"ğŸ“ IoU      : {iou:.4f}")
    print(f"ğŸ“ˆ Precision: {precision:.4f}")
    print(f"ğŸ“‰ Recall   : {recall:.4f}")
    print(f"âœ”ï¸ Accuracy : {accuracy:.4f}")
    print(f"ğŸ§ª Specificity: {specificity:.4f}")

    result = {
        "loss": avg_loss,
        "dice": dice,
        "iou": iou,
        "precision": precision,
        "recall": recall,
        "accuracy": accuracy,
        "specificity": specificity
    }
    return (result, y_true, y_pred) if return_y else result

# ==================================================
# ì‹œê°í™” í•¨ìˆ˜: í•™ìŠµ ì§„í–‰ì— ë”°ë¥¸ Loss, Dice ë³€í™”
# ==================================================

def plot_training_history(history, tag=None):
    if not history or "train_loss" not in history:
        print("âš ï¸ No training history to plot.")
        return

    epochs = range(1, len(history["train_loss"]) + 1)

    plt.figure(figsize=(12, 5))

    # Loss
    if "train_loss" in history and "val_loss" in history:
        plt.subplot(1, 2, 1)
        plt.plot(epochs, history["train_loss"], label="Train Loss")
        plt.plot(epochs, history["val_loss"], label="Val Loss")
        plt.title("Loss per Epoch")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    # Dice
    if "train_dice" in history and "val_dice" in history:
        plt.subplot(1, 2, 2)
        plt.plot(epochs, history["train_dice"], label="Train Dice")
        plt.plot(epochs, history["val_dice"], label="Val Dice")
        plt.title("Dice Score per Epoch")
        plt.xlabel("Epoch")
        plt.ylabel("Dice Score")
        plt.legend()
        plt.grid(True)

    # ì œëª©
    overall_title = "Training History"
    if tag:
        overall_title += f" â€” {tag}"
    plt.suptitle(overall_title)

    plt.tight_layout()
    plt.show()

# ==================================================
# ì‹œê°í™” í•¨ìˆ˜: í˜¼ë™í–‰ë ¬
# ==================================================

def plot_confusion_matrix(
    y_true,
    y_pred,
    labels=["background (0)", "lesion (1)"],
    normalize=None
):
    """
    í˜¼ë™í–‰ë ¬ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. normalize ì˜µì…˜ì„ í†µí•´ ì •ê·œí™”ë„ ê°€ëŠ¥.

    Args:
        y_true (array-like): ì •ë‹µ ë ˆì´ë¸”
        y_pred (array-like): ì˜ˆì¸¡ ë ˆì´ë¸”
        labels (list): í´ë˜ìŠ¤ ë¼ë²¨ ëª©ë¡
        normalize (str or None): 'true', 'pred', 'all', ë˜ëŠ” None ê°€ëŠ¥
    """
    valid_normalize = [None, "true", "pred", "all"]
    if normalize not in valid_normalize:
        print(f"âš ï¸ [Invalid normalize option] '{normalize}' â†’ Noneìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        normalize = None
    else:
        print(f"ğŸ§® Confusion matrix normalization: {normalize}")

    cm = confusion_matrix(y_true, y_pred, normalize=normalize)
    print(f"\nğŸ“Š Confusion Matrix (normalize={normalize}):\n{cm}")

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f")
    plt.title("Confusion Matrix")
    plt.show()

# ==================================================
# ì‹œê°í™” í•¨ìˆ˜: 5ê°œ ìƒ˜í”Œì—ì„œ segmentation ê²°ê³¼ ë¹„êµ
# ==================================================

def visualize_random_samples_arrays(model, processor, images_np, masks_np, num_samples=5):
    """
    numpy ë°°ì—´ë¡œ ë¡œë“œëœ (H,W) ë˜ëŠ” (H,W,1) ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”.
    images_np: (N,H,W) or (N,H,W,1) float32 0~1
    masks_np : (N,H,W) {0,1}
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device); model.eval()

    idxs = random.sample(range(len(images_np)), num_samples)
    fig, axes = plt.subplots(num_samples, 3, figsize=(10, 4*num_samples))

    if num_samples == 1:
        axes = [axes]

    for row, idx in enumerate(idxs):
        img  = images_np[idx].squeeze()          # (H,W)
        msk  = masks_np[idx].squeeze()           # (H,W)

        # í‘ë°± â†’ RGB   (0~1 â†’ 0~255 uint8)
        img_rgb = (np.repeat(img[..., None], 3, axis=-1) * 255).astype(np.uint8)
        pil_img = Image.fromarray(img_rgb)

        inputs = processor(images=pil_img, return_tensors="pt").to(device)
        with torch.no_grad():
            pred = model(**inputs).logits.argmax(dim=1).squeeze().cpu().numpy()

        axes[row][0].imshow(pil_img);    axes[row][0].set_title("Original")
        axes[row][1].imshow(msk,  cmap="gray"); axes[row][1].set_title("Ground-Truth")
        axes[row][2].imshow(pred, cmap="gray"); axes[row][2].set_title("Predicted")

        for ax in axes[row]:
            ax.axis("off")

    plt.tight_layout(); plt.show()

# ==================================================
# test setì˜ ëª¨ë“  imageì— ëŒ€í•´ diceì™€ lossë¥¼ ì¶”ì¶œ
# ==================================================

def export_per_sample_metrics(model, test_loader, save_csv_path=None):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    dice_list = []
    iou_list = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="ğŸ” Evaluating per sample"):
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs)
            preds = torch.argmax(outputs.logits, dim=1)

            # í•´ìƒë„ ë§ì¶”ê¸°
            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()

            for i in range(labels.shape[0]):
                y_true = labels[i].cpu().numpy().flatten()
                y_pred = preds[i].cpu().numpy().flatten()

                dice = f1_score(y_true, y_pred, zero_division=0)
                iou = jaccard_score(y_true, y_pred, zero_division=0)

                dice_list.append(dice)
                iou_list.append(iou)

    df = pd.DataFrame({
        "Dice": dice_list,
        "IoU": iou_list
    })

    if save_csv_path:
        df.to_csv(save_csv_path, index=False)
        print(f"âœ… CSV saved: {save_csv_path}")
    else:
        print("âš ï¸ CSV path not provided â€” DataFrame not saved.")

    return df

"""## HuggingFace SegFormer í•™ìŠµ ìˆ˜í–‰ ë° í…ŒìŠ¤íŠ¸"""

# ==================================================
# í•™ìŠµ ìˆ˜í–‰
# ==================================================

# í•™ìŠµ ìˆ˜í–‰ ì§ì „ ì „ì²˜ë¦¬ê¸° ê´€ë ¨ ì¤‘ìš” ì „ì—­ë³€ìˆ˜ ì¬ì„ ì–¸
current = experiment_configs["no_resize_128"]

# ì„ íƒëœ ì„¤ì •ì„ SegformerImageProcessorì— ì ìš©
image_processor = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

# í•™ìŠµ ìˆ˜í–‰ ì§ì „ ëª¨ë¸ ìˆ˜í–‰ ê´€ë ¨ ì¤‘ìš” ì „ì—­ë³€ìˆ˜ ì¬ì„ ì–¸
config = create_config_from_preprocessor(current, num_labels=2)
model = SegformerForSemanticSegmentation(config)

# í•™ìŠµ ìˆ˜í–‰ ì§ì „ ë°ì´í„°ì…‹ ê´€ë ¨ ì¤‘ìš” ì „ì—­ë³€ìˆ˜ ì¬ì„ ì–¸
train_loader, val_loader, _ = build_covid_dataloaders(
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    X_test=X_test,   # ë¬´ì‹œë¨
    y_test=y_test,   # ë¬´ì‹œë¨
    processor=image_processor,
    batch_size=8,
    num_workers=4,
    debug=True
)

# ì¡°ì‘ ê°€ëŠ¥í•œ í•™ìŠµì„¤ì •
train_config = {
    "batch_size": 8,
    "lr": 5e-5,
    "num_epochs": 20,
    "weight_decay": 3e-4,
    "early_stopping_metric": "loss",
    "patience": 7
}

# ìœ„ì˜ í•™ìŠµì„¤ì • ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµìˆ˜í–‰ í•¨ìˆ˜ í˜¸ì¶œ
model, history = train_segformer_model(
    model=model,
    image_processor=image_processor,
    train_loader=train_loader,
    val_loader=val_loader,
    tag=current["name"],
    **train_config
)

# ==================================================
# ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ í›„ ì„±ëŠ¥ì§€í‘œ í™•ì¸ ë° ì‹œê°í™”
# ==================================================

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ
saved_model_path = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/SegFormer-b0-no-resize-128_0624_1112"

# ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ
base_model = SegformerForSemanticSegmentation.from_pretrained(saved_model_path)
model_loaded = SegFormerWithUpsample(base_model, upsample_size=(128, 128))
model_loaded.eval()

# 3) ì „ì²˜ë¦¬ê¸° ë³µì› (í•™ìŠµ ë‹¹ì‹œ configì—ì„œ ì¬ìƒì„±)
current = experiment_configs["no_resize_128"]
image_processor_loaded = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

# ëª¨ë¸ ì‹ë³„ìš© íƒœê·¸ ì¶”ì¶œ
extracted_tag = os.path.basename(saved_model_path)

# í…ŒìŠ¤íŠ¸ìš© DataLoader êµ¬ì„±
_, _, test_loader = build_covid_dataloaders(
    X_train, y_train,
    X_val,   y_val,
    X_test,  y_test,
    processor=image_processor_loaded,
    batch_size=8,
    debug=False,
    num_workers=4
)

# ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
test_metrics, y_true, y_pred = evaluate_on_testset(
    model=model_loaded,
    test_loader=test_loader,
    tag=extracted_tag,
    return_y=True
)

# ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
plot_training_history(history, tag=extracted_tag)
plot_confusion_matrix(y_true, y_pred)
visualize_random_samples_arrays(
    model_loaded,
    image_processor_loaded,
    X_test,
    y_test,
    num_samples=5
)

# dice, loss ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ
df_sample = export_per_sample_metrics(
    model       = model_loaded,
    test_loader = test_loader,
    save_csv_path = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/SegFormer-b0-no-resize-128_0624_1112/per_sample_metrics.csv"
)