# -*- coding: utf-8 -*-
"""TransUNet_whole_chest_seg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v0hpbxAbdpgJmyF-kqaxQmSq9OLK07BI

## 필수 패키지/라이브러리 설치
"""

!pip install segmentation-models-pytorch

# ============================
# 1. 필요한 라이브러리 import
# ============================
import os, json
from pathlib import Path           # ★ Path
from typing import Tuple, Dict, List   # ★ Tuple, Dict, List
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim        # ★ optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
)
from torchvision.models import resnet50
import matplotlib.pyplot as plt
import seaborn as sns
import segmentation_models_pytorch as smp
from collections import Counter
from typing import Tuple, Dict, List
import torch.nn.functional as F

"""## 데이터 로드"""

#### 데이터 불러오기
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

# 파일 경로 지정
DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/raw_data/chest_x_ray'

paths = {
    'train_X': os.path.join(DATA_DIR, 'chest_X_train.npy'),
    'train_y': os.path.join(DATA_DIR, 'chest_y_train.npy'),
    'val_X'  : os.path.join(DATA_DIR, 'chest_X_val.npy'),
    'val_y'  : os.path.join(DATA_DIR, 'chest_y_val.npy'),
    'test_X' : os.path.join(DATA_DIR, 'chest_X_test.npy'),
    'test_y' : os.path.join(DATA_DIR, 'chest_y_test.npy'),
}

# 파일 존재 여부 확인
print("\n\n" + "=" * 50)
print("🔍 .npy Dataset Path & File Check")
print("=" * 50)

for name, path in paths.items():
    exists = os.path.exists(path)
    print(f"{name:10s} → {path} | Exists: {exists}")

# .npy 파일 로딩
X_train = np.load(paths['train_X'], allow_pickle=True)
y_train = np.load(paths['train_y'], allow_pickle=True)
X_val   = np.load(paths['val_X'],   allow_pickle=True)
y_val   = np.load(paths['val_y'],   allow_pickle=True)
X_test  = np.load(paths['test_X'],  allow_pickle=True)
y_test  = np.load(paths['test_y'],  allow_pickle=True)

# 데이터셋 크기 출력
print("\n\n" + "=" * 50)
print("🔍 Dataset Directory & File Count Check")
print("=" * 50)

print(f"Train 👉 Images: {X_train.shape}, Masks: {y_train.shape}")
print(f"Valid 👉 Images: {X_val.shape}, Masks: {y_val.shape}")
print(f"Test  👉 Images: {X_test.shape}, Masks: {y_test.shape}")

# ==================================================
# 이미지/마스크 쌍의 파일명, 크기 및 채널 정합성 확인
# ==================================================

def check_array_pair_consistency(images: np.ndarray,
                                 masks: np.ndarray,
                                 set_name: str = "Set") -> None:

    print("\n" + "=" * 50)
    print(f"🔍 Consistency Check for {set_name}")
    print("=" * 50)

    # Sample count
    if len(images) != len(masks):
        print(f"[❗] Sample count mismatch: {len(images)} images vs {len(masks)} masks")
    else:
        print(f"[✅] Sample count: {len(images)} (matched)")

    # Shape mismatch detection
    shape_mismatches = []
    for i, (img, msk) in enumerate(zip(images, masks)):
        if img.shape != msk.shape:
            shape_mismatches.append((i, img.shape, msk.shape))

    if shape_mismatches:
        print(f"[❗] Found {len(shape_mismatches)} shape mismatches:")
        for idx, img_shape, mask_shape in shape_mismatches[:5]:
            print(f"   - Index {idx}: image={img_shape}, mask={mask_shape}")
        if len(shape_mismatches) > 5:
            print("   ...")
    else:
        print("[✅] All image/mask shapes match.")

    # Channel count summary (last dim)
    def get_channel_count(arr):
        return arr.shape[-1] if arr.ndim == 4 else 1

    img_chan = get_channel_count(images)
    mask_chan = get_channel_count(masks)

    if img_chan != mask_chan:
        print(f"[❗] Channel count mismatch: image={img_chan}, mask={mask_chan}")
    else:
        print(f"[✅] Channel count: {img_chan} (matched)")

# Run consistency checks
check_array_pair_consistency(X_train, y_train, "Train Set")
check_array_pair_consistency(X_val, y_val, "Validation Set")
check_array_pair_consistency(X_test, y_test, "Test Set")

# ==================================================
# 사전 진단: 255 포함 마스크 수 확인
# ==================================================

def count_masks_with_255(masks: np.ndarray, set_name: str = "Set"):

    print("\n" + "=" * 50)
    print(f"🔍 Checking 255 presence in {set_name}")
    print("=" * 50)

    # Remove last dimension if it's singleton (e.g., shape: (N, H, W, 1))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    # Vectorized check: (mask == 255).any over (H, W)
    count = int((masks == 255).any(axis=(1, 2)).sum())
    total = len(masks)

    print(f"⚠️  Masks with 255: {count} / {total}")

# Run the check for all sets
count_masks_with_255(y_train, "Train Set")
count_masks_with_255(y_val,   "Validation Set")
count_masks_with_255(y_test,  "Test Set")

# ==================================================
# 마스크 픽셀값 분포 확인하기
# ==================================================

def analyze_mask_pixel_distribution_np(masks: np.ndarray,
                                       set_name: str = "Set"):

    print("\n" + "="*50)
    print(f"🔍 Analysing pixel-value distribution — {set_name}")
    print("="*50)

    # Squeeze last dim if singleton (e.g., (N, H, W, 1) → (N, H, W))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    unique_sets = [tuple(sorted(np.unique(m))) for m in masks]
    value_counter = Counter(unique_sets)

    all_binary = all(len(u) == 2 for u in unique_sets)
    common_set = unique_sets[0]
    all_same = all_binary and all(u == common_set for u in unique_sets)

    # print summary
    print(f"Total masks analysed        : {len(masks)}")
    print(f"Are all masks binary (2 vals)?  {'Yes' if all_binary else 'No'}")
    print(f"Do all masks share same vals?   {'Yes' if all_same else 'No'}")

    print("\n📊 Pixel-value combinations across masks:")
    for vals, cnt in value_counter.items():
        print(f"  {vals} → {cnt} mask(s)")

    return {
        "all_binary": all_binary,
        "all_same": all_same,
        "value_counter": value_counter
    }

# Run on each split
train_summary = analyze_mask_pixel_distribution_np(y_train, "Train Set")
val_summary   = analyze_mask_pixel_distribution_np(y_val,   "Validation Set")
test_summary  = analyze_mask_pixel_distribution_np(y_test,  "Test Set")

# ==================================================
# 이미지 크기 통계 수집
# ==================================================

def get_dims_from_arrays(arr: np.ndarray):
    if arr.ndim == 4:       # (N, H, W, C)
        _, H, W, _ = arr.shape
    elif arr.ndim == 3:     # (N, H, W)
        _, H, W = arr.shape
    else:
        raise ValueError("Array must be 3-D or 4-D (batch of images).")

    # All samples share the same H and W here, but we still create per-sample arrays
    # in case your future dataset mixes sizes.
    widths  = np.full(len(arr), W)
    heights = np.full(len(arr), H)
    return widths, heights

# Printer
def print_size_stats(name: str, widths: np.ndarray, heights: np.ndarray):
    ratios = widths / heights
    print("\n" + "=" * 60)
    print(f"🖼️  {name} — Size Statistics")
    print("=" * 60)
    print(f"Samples               : {len(widths)}")
    print(f"Width   → min {widths.min()},  max {widths.max()},  "
          f"mean {widths.mean():.2f},  median {np.median(widths)}")
    print(f"Height  → min {heights.min()}, max {heights.max()}, "
          f"mean {heights.mean():.2f}, median {np.median(heights)}")
    print(f"Ratio W/H → min {ratios.min():.3f}, max {ratios.max():.3f}, "
          f"mean {ratios.mean():.3f}, median {np.median(ratios):.3f}")

# Analyse any split you like
def analyse_split(images: np.ndarray, masks: np.ndarray, split: str):
    W_img, H_img = get_dims_from_arrays(images)
    W_msk, H_msk = get_dims_from_arrays(masks)

    print_size_stats(f"Images — {split}", W_img, H_img)
    print_size_stats(f"Masks  — {split}", W_msk, H_msk)

# Run on each set
analyse_split(X_train, y_train, "Train")
analyse_split(X_val,   y_val,   "Validation")
analyse_split(X_test,  y_test,  "Test")

"""## TransUNet Dataset/DataLoader/모델생성 관련 함수"""

# ============================
# Dataset Class 정의
# ============================

class NumpyDataset(Dataset):
    def __init__(self, images, masks):
        self.images = images
        self.masks = masks

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        x = self.images[idx]
        if x.ndim == 3 and x.shape[-1] == 1:
            x = x[..., 0]  # (H, W)로 만들어줌
        x = x[np.newaxis, ...]  # (1, H, W)

        y = self.masks[idx]
        if y.ndim == 3 and y.shape[-1] == 1:
            y = y[..., 0]  # (H, W)

        return torch.FloatTensor(x), torch.LongTensor(y)

# ============================
# DataLoader 정의
# ============================

def get_dataloaders(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    batch_size: int = 8,
) -> Tuple[DataLoader, DataLoader, DataLoader]:

    train_loader = DataLoader(
        NumpyDataset(X_train, y_train), batch_size=batch_size, shuffle=True
    )
    val_loader = DataLoader(NumpyDataset(X_val, y_val), batch_size=batch_size)
    test_loader = DataLoader(NumpyDataset(X_test, y_test), batch_size=1)
    return train_loader, val_loader, test_loader

# ============================
# 모델 생성
# ============================

def build_model():
    return TransUNet(
        img_size=16,
        in_channels=1,
        num_classes=1,
        patch_size=4,
        embed_dim=768,
        depth=8,
        num_heads=12
    )

# ============================================================
# TransUNet: ResNet-50 Encoder + Transformer + Decoder
# ============================================================

class PatchEmbedding(nn.Module):
    def __init__(self, in_channels, embed_dim, patch_size, img_size):
        super().__init__()
        self.patch_size = patch_size
        self.grid_size = img_size // patch_size
        self.num_patches = self.grid_size ** 2

        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        nn.init.trunc_normal_(self.pos_embed, std=0.02)

    def forward(self, x):
        x = self.proj(x)  # (B, D, H', W') where H', W' = H/patch_size
        B, D, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)  # (B, N=H'*W', D)
        if x.size(1) != self.pos_embed.size(1):
            raise ValueError(f"Mismatch in patch count: x has {x.size(1)} patches, pos_embed expects {self.pos_embed.size(1)}")
        return x + self.pos_embed.to(x.device)


class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x


class TransUNet(nn.Module):
    def __init__(self, img_size=16, in_channels=1, num_classes=1, patch_size=4, embed_dim=768, depth=4, num_heads=12):
        super().__init__()

        # Encoder (ResNet-50)
        backbone = resnet50(weights="IMAGENET1K_V1")
        self.input_adjust = nn.Conv2d(in_channels, 3, 1) if in_channels != 3 else nn.Identity()
        self.encoder1 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu, backbone.maxpool)
        self.encoder2 = backbone.layer1
        self.encoder3 = backbone.layer2  # output size: 16x16 when input is 128x128

        # Transformer bridge
        self.patch_embed = PatchEmbedding(
            in_channels=512,
            embed_dim=embed_dim,
            patch_size=patch_size,
            img_size=img_size  # = 16, the size of encoder3 feature map
        )
        self.grid_size = self.patch_embed.grid_size
        self.transformer = nn.Sequential(
            *[TransformerBlock(embed_dim, num_heads) for _ in range(depth)]
        )

        # Decoder: 16→32→64→128
        self.up1 = nn.ConvTranspose2d(embed_dim, 256, 2, stride=2)
        self.decoder1 = nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),
        )
        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.decoder2 = nn.Sequential(
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(),
        )
        self.up3 = nn.ConvTranspose2d(32, 16, 2, stride=2)
        self.out_conv = nn.Conv2d(16, num_classes, 1)

    def forward(self, x):
        x = self.input_adjust(x)
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(enc1)
        enc3 = self.encoder3(enc2)

        B, C, H, W = enc3.shape
        x = self.patch_embed(enc3)
        x = self.transformer(x)
        x = x.transpose(1, 2).reshape(B, -1, self.grid_size, self.grid_size)  # [B, embed_dim, 16, 16]

        x = self.up1(x)  # 16 → 32
        x = F.interpolate(x, size=enc2.shape[-2:], mode="bilinear", align_corners=False)
        x = torch.cat([x, enc2], dim=1)
        x = self.decoder1(x)

        x = self.up2(x)  # 32 → 64
        x = self.decoder2(x)

        x = self.up3(x)  # 64 → 128
        x = self.out_conv(x)  # final: [B, 1, 128, 128]
        return x

# ============================
# Metric Functions
# ============================
def compute_metrics(y_true, y_pred):
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)

    dice = 2 * tp / (2 * tp + fp + fn + 1e-8)
    iou = tp / (tp + fp + fn + 1e-8)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    specificity = tn / (tn + fp + 1e-8)
    accuracy = accuracy_score(y_true, y_pred)

    return dice, iou, precision, recall, specificity, accuracy, cm

"""## TransUNet 학습 관련 함수"""

# ============================
# 학습 준비 (학습 루프에서 호출됨)
# ============================

def setup_training(model: nn.Module, lr: float = 1e-4) -> Tuple[nn.Module, torch.optim.Optimizer, nn.Module, Dict[str, List[float]]]:
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_dice": [],
        "val_dice": [],
    }
    return model, optimizer, criterion, history

# ============================
# 학습 루프
# ============================

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: torch.device,
    save_dir: str,
    epochs: int = 20,
    lr: float = 1e-4,
    patience: int = 5,
) -> Tuple[nn.Module, Dict[str, List[float]]]:

    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    model, optimizer, criterion, history = setup_training(model, lr)

    best_val_loss = float("inf")
    patience_counter = 0

    for epoch in range(epochs):
        # --------------------
        # Train phase
        # --------------------
        model.train()
        running_loss = 0.0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device).float()
            pred = model(x)
            loss = criterion(pred.squeeze(1), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        avg_train_loss = running_loss / len(train_loader)

        # Compute train dice
        model.eval()
        train_preds, train_labels = [], []
        with torch.no_grad():
            for x, y in train_loader:
                x, y = x.to(device), y.to(device).float()
                prob = torch.sigmoid(model(x)).squeeze(1).cpu().numpy()
                train_preds.append((prob > 0.5).astype(np.uint8))
                train_labels.append(y.cpu().numpy())
        train_dice = compute_metrics(
            np.concatenate(train_labels).flatten(),
            np.concatenate(train_preds).flatten(),
        )[0]

        # --------------------
        # Validation phase
        # --------------------
        val_loss, val_preds, val_labels = 0.0, [], []
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device).float()
                pred = model(x)
                loss = criterion(pred.squeeze(1), y)
                val_loss += loss.item()
                prob = torch.sigmoid(pred.squeeze(1)).cpu().numpy()
                val_preds.append((prob > 0.5).astype(np.uint8))
                val_labels.append(y.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        val_dice = compute_metrics(
            np.concatenate(val_labels).flatten(),
            np.concatenate(val_preds).flatten(),
        )[0]

        # --------------------
        # History update
        # --------------------
        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(avg_val_loss)
        history["train_dice"].append(train_dice)
        history["val_dice"].append(val_dice)

        print(
            f"Epoch {epoch+1:02}/{epochs} | "
            f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | "
            f"Train Dice: {train_dice:.4f} | Val Dice: {val_dice:.4f}"
        )

        # --------------------
        # Checkpoint by val_loss
        # --------------------
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), save_path / "best_model.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break

    # Save history as JSON for convenience
    with open(save_path / "history.json", "w", encoding="utf-8") as fp:
        json.dump(history, fp, indent=2)

    # Reload best weights before returning
    model.load_state_dict(torch.load(save_path / "best_model.pth", map_location=device))
    return model, history

# ============================
# 학습 호출
# ============================

def run_training(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    save_dir: str,
    batch_size: int = 8,
    epochs: int = 20,
    lr: float = 1e-4,
    patience: int = 5,
) -> Tuple[nn.Module, Dict[str, List[float]]]:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train_loader, val_loader, _ = get_dataloaders(X_train, y_train, X_val, y_val, X_val, y_val, batch_size=batch_size)
    model = build_model().to(device)
    return train_model(
        model,
        train_loader,
        val_loader,
        device,
        save_dir=save_dir,
        epochs=epochs,
        lr=lr,
        patience=patience,
    )

# ============================
# 평가 함수
# ============================

def evaluate_model(
    model: nn.Module,
    test_loader: DataLoader,
    device: torch.device,
    save_dir: str = "./output",
    csv_name: str = "per_image_metrics.csv",
):
    """Evaluate model on test set, print metrics and save confusion matrix & per-image metrics."""

    model.eval()
    preds, labels = [], []
    per_image_metrics = []             # 👈 여기

    with torch.no_grad():
        for idx, (x, y) in enumerate(test_loader):
            x, y = x.to(device), y.to(device).float()
            prob = torch.sigmoid(model(x)).squeeze(1).cpu().numpy()
            pred_bin = (prob > 0.5).astype(np.uint8)

            # ── 전체 평가용 누적 ─────────────────
            preds.append(pred_bin)
            labels.append(y.cpu().numpy())

            # ── 개별 이미지 Dice / IoU ───────────
            dice, iou = dice_iou_single(y.cpu().numpy()[0], pred_bin[0])
            per_image_metrics.append(
                {"image_idx": idx, "dice": float(dice), "iou": float(iou)}
            )

    # ───────── 전체 메트릭 (기존 코드) ───────────
    y_true = np.concatenate([a.flatten() for a in labels])
    y_pred = np.concatenate([a.flatten() for a in preds])

    dice, iou, prec, rec, spec, acc, cm = compute_metrics(y_true, y_pred)
    print(
        "\n📊 Test-set Evaluation\n"
        f"Dice: {dice:.4f}, IoU: {iou:.4f}, Precision: {prec:.4f}, "
        f"Recall: {rec:.4f}, Specificity: {spec:.4f}, Accuracy: {acc:.4f}"
    )

    # ───────── confusion matrix 저장 ────────────
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix")
    plt.savefig(save_path / "confusion_matrix.png")
    plt.close()

    # ───────── per-image CSV 저장 ───────────────
    df = pd.DataFrame(per_image_metrics)
    df.reset_index(drop=True, inplace=True)
    csv_path = save_path / csv_name
    df.to_csv(csv_path, index=False)
    print(f"✅ Per-image Dice/IoU saved to {csv_path}")

    return {
        "dice": dice,
        "iou": iou,
        "precision": prec,
        "recall": rec,
        "specificity": spec,
        "accuracy": acc,
        "confusion_matrix": cm.tolist(),
        "per_image_path": str(csv_path),
    }

def dice_iou_single(mask_true: np.ndarray, mask_pred: np.ndarray, eps: float = 1e-8):
    """단일 이미지 Dice / IoU 계산 (binary)."""
    mask_true = mask_true.astype(bool)
    mask_pred = mask_pred.astype(bool)

    intersection = np.logical_and(mask_true, mask_pred).sum()
    union        = np.logical_or(mask_true,  mask_pred).sum()
    dice = (2 * intersection + eps) / (mask_true.sum() + mask_pred.sum() + eps)
    iou  = (intersection + eps) / (union + eps)
    return dice, iou

"""## TrainUNet"""

import numpy as np

# 저장 디렉토리 지정 (외부 주입)
save_dir = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/trans_unet"

# 학습 수행
model, history = run_training(
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    save_dir=save_dir,
    batch_size=8,
    epochs=20,
    lr=1e-4,
    patience=5
)

# ============================
# 학습과정 시각화를 위한 모듈 불러오기 (Loss, Dice)
# ============================

# 모듈화된 함수 임포트 및 함수 docstring 불러오기
import sys
import importlib

# 모듈 경로 추가 (이미 한 번 했더라도 문제 없음)
sys.path.append('/content/drive/MyDrive/Colab Notebooks/tools_global')

# 모듈 전체 import -> 재로딩 -> 함수 꺼내기
import plot_training_metrics
importlib.reload(plot_training_metrics)
from plot_training_metrics import plot_training_metrics

# 도움말 확인 (optional)
# help(plot_training_metrics)

# 저장된 history 불러오기
with open("/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/trans_unet/history.json", "r") as f:
    history = json.load(f)

# 시각화
plot_training_metrics(
    history = history,
    tag = "TransUNet",
    metrics = ["loss", "dice"],
    filter_mode = "paired",
    figsize = (12, 5),
    layout = "horizontal",
    titles={
        "loss": "Loss",
        "dice": "Dice Coefficient",
    },
    # xlim=(1, 20),
    # ylims={
    #     "loss": (0.0, 0.5),
    #     "dice": (0.5, 1.0),
    # },
    grid=True,
    use_tight_layout=True,
)

# ============================
# 학습된 모델 불러와서 테스트
# ============================

# 테스트용 데이터로더
_, _, test_loader = get_dataloaders(
    X_train=X_test, y_train=y_test,
    X_val=X_test, y_val=y_test,
    X_test=X_test, y_test=y_test,
    batch_size=1
)

# 모델 로드 및 평가
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = build_model().to(device)
model_path = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/trans_unet/best_model.pth"
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()

# 평가 함수 실행
test_metrics = evaluate_model(
    model=model,
    test_loader=test_loader,
    device=device,
    save_dir="/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/trans_unet"
)

# 출력 확인
print("\n✅ Test-set Metrics:")
for k, v in test_metrics.items():
    if k == "confusion_matrix":
        continue
    if isinstance(v, float):
        print(f"{k}: {v:.4f}")
    else:
        print(f"{k}: {v}")

import matplotlib.pyplot as plt
import torch
import numpy as np

def show_predictions(model, test_loader, device, num_samples=5, threshold=0.5):
    model.eval()
    samples_shown = 0

    plt.figure(figsize=(12, num_samples * 5))

    with torch.no_grad():
        for i, (x, y_true) in enumerate(test_loader):
            if samples_shown >= num_samples:
                break

            x = x.to(device)
            pred = torch.sigmoid(model(x)).squeeze(1).cpu().numpy()[0]
            pred_mask = (pred > threshold).astype(np.uint8)

            image = x.cpu().numpy()[0, 0]
            gt_mask = y_true.numpy()[0]

            # Plot
            idx = samples_shown
            plt.subplot(num_samples, 3, idx * 3 + 1)
            plt.imshow(image, cmap="gray")
            plt.title("Input Image")
            plt.axis("off")

            plt.subplot(num_samples, 3, idx * 3 + 2)
            plt.imshow(gt_mask, cmap="gray")
            plt.title("Ground Truth")
            plt.axis("off")

            plt.subplot(num_samples, 3, idx * 3 + 3)
            plt.imshow(pred_mask, cmap="gray")
            plt.title("Predicted Mask")
            plt.axis("off")

            samples_shown += 1

    plt.tight_layout()
    plt.show()

show_predictions(model, test_loader=test_loader, device=device, num_samples=5)
