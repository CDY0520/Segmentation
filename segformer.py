# -*- coding: utf-8 -*-
"""SegFormer_whole_lung_seg_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvXQGjDYJVPkvsZS9vmuG_RdAWRpRk7U

## Colab 환경설정
"""

# ==================================================
# Google Drive 마운트
# ==================================================

from google.colab import drive
drive.mount('/content/drive')

# ==================================================
# Google Drive 데이터 확인 오류 해결
# ==================================================

# # "image file is truncated" 오류 무시하고 모두 불러오기
# ImageFile.LOAD_TRUNCATED_IMAGES = True

# ==================================================
# 필수 패키지 설치 (colab 기준)
# ==================================================

!pip install transformers accelerate torchvision
!pip install optuna

"""## 필수 라이브러리 임포트"""

# 표준 라이브러리
import os
import copy
import zipfile
import pickle
import random
import pytz
from glob import glob
from datetime import datetime, timedelta, timezone
from collections import Counter
from multiprocessing import Pool
from os.path import basename, splitext

# 데이터 처리 및 시각화
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# 머신러닝 & 평가 지표
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    jaccard_score,
    precision_score,
    recall_score,
)

# PyTorch & TorchVision
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import torch.nn as nn
import torch.nn.functional as F

# HuggingFace Transformers
from transformers import (
    SegformerConfig,
    SegformerForSemanticSegmentation,
    SegformerImageProcessor,
)

# 하이퍼파라미터 튜닝
import optuna

# Colab 전용
from google.colab import drive

"""## 데이터 불러오기 및 속성 파악하기"""

# ==================================================
# 경로 설정 및 파일 리스트 생성
# ==================================================

# 파일 경로 지정
DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/projects/kdt_med_seg/raw_data/chest_x_ray'

paths = {
    'train_X': os.path.join(DATA_DIR, 'X_train.npy'),
    'train_y': os.path.join(DATA_DIR, 'y_train.npy'),
    'val_X'  : os.path.join(DATA_DIR, 'X_val.npy'),
    'val_y'  : os.path.join(DATA_DIR, 'y_val.npy'),
    'test_X' : os.path.join(DATA_DIR, 'X_test.npy'),
    'test_y' : os.path.join(DATA_DIR, 'y_test.npy'),
}

# 파일 존재 여부 확인
print("\n\n" + "=" * 50)
print("🔍 .npy Dataset Path & File Check")
print("=" * 50)

for name, path in paths.items():
    exists = os.path.exists(path)
    print(f"{name:10s} → {path} | Exists: {exists}")

# .npy 파일 로딩
train_images = np.load(paths['train_X'])
train_masks  = np.load(paths['train_y'])

val_images = np.load(paths['val_X'])
val_masks  = np.load(paths['val_y'])

test_images = np.load(paths['test_X'])
test_masks  = np.load(paths['test_y'])

# 데이터셋 크기 출력
print("\n\n" + "=" * 50)
print("🔍 Dataset Directory & File Count Check")
print("=" * 50)

print(f"Train 👉 Images: {train_images.shape}, Masks: {train_masks.shape}")
print(f"Valid 👉 Images: {val_images.shape}, Masks: {val_masks.shape}")
print(f"Test  👉 Images: {test_images.shape}, Masks: {test_masks.shape}")

# ==================================================
# 이미지/마스크 쌍의 파일명, 크기 및 채널 정합성 확인
# ==================================================

def check_array_pair_consistency(images: np.ndarray,
                                 masks: np.ndarray,
                                 set_name: str = "Set") -> None:

    print("\n" + "=" * 50)
    print(f"🔍 Consistency Check for {set_name}")
    print("=" * 50)

    # Sample count
    if len(images) != len(masks):
        print(f"[❗] Sample count mismatch: {len(images)} images vs {len(masks)} masks")
    else:
        print(f"[✅] Sample count: {len(images)} (matched)")

    # Shape mismatch detection
    shape_mismatches = []
    for i, (img, msk) in enumerate(zip(images, masks)):
        if img.shape != msk.shape:
            shape_mismatches.append((i, img.shape, msk.shape))

    if shape_mismatches:
        print(f"[❗] Found {len(shape_mismatches)} shape mismatches:")
        for idx, img_shape, mask_shape in shape_mismatches[:5]:
            print(f"   - Index {idx}: image={img_shape}, mask={mask_shape}")
        if len(shape_mismatches) > 5:
            print("   ...")
    else:
        print("[✅] All image/mask shapes match.")

    # Channel count summary (last dim)
    def get_channel_count(arr):
        return arr.shape[-1] if arr.ndim == 4 else 1

    img_chan = get_channel_count(images)
    mask_chan = get_channel_count(masks)

    if img_chan != mask_chan:
        print(f"[❗] Channel count mismatch: image={img_chan}, mask={mask_chan}")
    else:
        print(f"[✅] Channel count: {img_chan} (matched)")

# Run consistency checks
check_array_pair_consistency(train_images, train_masks, "Train Set")
check_array_pair_consistency(val_images, val_masks, "Validation Set")
check_array_pair_consistency(test_images, test_masks, "Test Set")

# ==================================================
# 사전 진단: 255 포함 마스크 수 확인
# ==================================================

def count_masks_with_255(masks: np.ndarray, set_name: str = "Set"):

    print("\n" + "=" * 50)
    print(f"🔍 Checking 255 presence in {set_name}")
    print("=" * 50)

    # Remove last dimension if it's singleton (e.g., shape: (N, H, W, 1))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    # Vectorized check: (mask == 255).any over (H, W)
    count = int((masks == 255).any(axis=(1, 2)).sum())
    total = len(masks)

    print(f"⚠️  Masks with 255: {count} / {total}")

# Run the check for all sets
count_masks_with_255(train_masks, "Train Set")
count_masks_with_255(val_masks,   "Validation Set")
count_masks_with_255(test_masks,  "Test Set")

# ==================================================
# 마스크 픽셀값 분포 확인하기
# ==================================================

def analyze_mask_pixel_distribution_np(masks: np.ndarray,
                                       set_name: str = "Set"):

    print("\n" + "="*50)
    print(f"🔍 Analysing pixel-value distribution — {set_name}")
    print("="*50)

    # Squeeze last dim if singleton (e.g., (N, H, W, 1) → (N, H, W))
    if masks.ndim == 4 and masks.shape[-1] == 1:
        masks = masks.squeeze(-1)

    unique_sets = [tuple(sorted(np.unique(m))) for m in masks]
    value_counter = Counter(unique_sets)

    all_binary = all(len(u) == 2 for u in unique_sets)
    common_set = unique_sets[0]
    all_same = all_binary and all(u == common_set for u in unique_sets)

    # print summary
    print(f"Total masks analysed        : {len(masks)}")
    print(f"Are all masks binary (2 vals)?  {'Yes' if all_binary else 'No'}")
    print(f"Do all masks share same vals?   {'Yes' if all_same else 'No'}")

    print("\n📊 Pixel-value combinations across masks:")
    for vals, cnt in value_counter.items():
        print(f"  {vals} → {cnt} mask(s)")

    return {
        "all_binary": all_binary,
        "all_same": all_same,
        "value_counter": value_counter
    }

# Run on each split
train_summary = analyze_mask_pixel_distribution_np(train_masks, "Train Set")
val_summary   = analyze_mask_pixel_distribution_np(val_masks,   "Validation Set")
test_summary  = analyze_mask_pixel_distribution_np(test_masks,  "Test Set")

# ==================================================
# 이미지 크기 통계 수집
# ==================================================

def get_dims_from_arrays(arr: np.ndarray):
    if arr.ndim == 4:       # (N, H, W, C)
        _, H, W, _ = arr.shape
    elif arr.ndim == 3:     # (N, H, W)
        _, H, W = arr.shape
    else:
        raise ValueError("Array must be 3-D or 4-D (batch of images).")

    # All samples share the same H and W here, but we still create per-sample arrays
    # in case your future dataset mixes sizes.
    widths  = np.full(len(arr), W)
    heights = np.full(len(arr), H)
    return widths, heights

# Printer
def print_size_stats(name: str, widths: np.ndarray, heights: np.ndarray):
    ratios = widths / heights
    print("\n" + "=" * 60)
    print(f"🖼️  {name} — Size Statistics")
    print("=" * 60)
    print(f"Samples               : {len(widths)}")
    print(f"Width   → min {widths.min()},  max {widths.max()},  "
          f"mean {widths.mean():.2f},  median {np.median(widths)}")
    print(f"Height  → min {heights.min()}, max {heights.max()}, "
          f"mean {heights.mean():.2f}, median {np.median(heights)}")
    print(f"Ratio W/H → min {ratios.min():.3f}, max {ratios.max():.3f}, "
          f"mean {ratios.mean():.3f}, median {np.median(ratios):.3f}")

# Analyse any split you like
def analyse_split(images: np.ndarray, masks: np.ndarray, split: str):
    W_img, H_img = get_dims_from_arrays(images)
    W_msk, H_msk = get_dims_from_arrays(masks)

    print_size_stats(f"Images — {split}", W_img, H_img)
    print_size_stats(f"Masks  — {split}", W_msk, H_msk)

# Run on each set
analyse_split(train_images, train_masks, "Train")
analyse_split(val_images,   val_masks,   "Validation")
analyse_split(test_images,  test_masks,  "Test")

"""## 훈련/검증/테스트 세트 분할"""

# ==================================================
# 분할된 데이터 다시 불러온 뒤 shape 확인
# ==================================================

DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/projects/kdt_med_seg/raw_data/chest_x_ray'

paths = {
    'train_X': os.path.join(DATA_DIR, 'X_train.npy'),
    'train_y': os.path.join(DATA_DIR, 'y_train.npy'),
    'val_X'  : os.path.join(DATA_DIR, 'X_val.npy'),
    'val_y'  : os.path.join(DATA_DIR, 'y_val.npy'),
    'test_X' : os.path.join(DATA_DIR, 'X_test.npy'),
    'test_y' : os.path.join(DATA_DIR, 'y_test.npy'),
}

# ==================================================
# 2. .npy 파일 로딩
# ==================================================
X_train = np.load(paths['train_X'], allow_pickle=True)
y_train = np.load(paths['train_y'], allow_pickle=True)
X_val   = np.load(paths['val_X'],   allow_pickle=True)
y_val   = np.load(paths['val_y'],   allow_pickle=True)
X_test  = np.load(paths['test_X'],  allow_pickle=True)
y_test  = np.load(paths['test_y'],  allow_pickle=True)

print("\n📊 Loaded Dataset Shapes:")
print(f"Train: X = {X_train.shape}, y = {y_train.shape}")
print(f"Val  : X = {X_val.shape},   y = {y_val.shape}")
print(f"Test : X = {X_test.shape},  y = {y_test.shape}")

"""## HuggingFace SegFormer 전처리기 설정


"""

# ==================================================
# X_train image의 RGB 기준 픽셀값의 mean/std 산출
# ==================================================

def compute_rgb_mean_std_from_npy(images: np.ndarray):

    # dtype·스케일 정규화
    if images.dtype == np.uint8:
        images = images.astype(np.float32) / 255.0  # 0~1 변환
    else:
        images = images.astype(np.float32)

    # 채널·shape 정리
    if images.ndim == 4 and images.shape[-1] == 1:         # (N, H, W, 1) → 흑백
        images = images.squeeze(-1)                        # (N, H, W)
    if images.ndim == 3:                                   # (N, H, W) ← 흑백
        # 흑백 → RGB 3채널 복제 (SegFormer 입력 호환)
        images = np.repeat(images[:, :, :, None], 3, axis=-1)  # (N, H, W, 3)

    # 2) 채널별 mean/std 계산 (벡터화)
    mean = images.mean(axis=(0, 1, 2))   # shape (3,)
    std  = images.std(axis=(0, 1, 2))    # shape (3,)

    # 3) 출력
    mean_list = mean.tolist()
    std_list  = std.tolist()
    print(f"📌 Train mean : {mean_list}")
    print(f"📌 Train std  : {std_list}")
    return mean_list, std_list

# 사용 예시
train_mean, train_std = compute_rgb_mean_std_from_npy(X_train)

# ==================================================
# 전처리기에 적용할 config 모음
# ==================================================

# 개별 config 선언
# resize = False인 경우 size는 무시
SegFormer_config_no_resize = {
    "name": "SegFormer-b0-no-resize-128",
    "model_name": "nvidia/segformer-b0",
    "resize": False,
    "size": {"height": 128, "width": 128},
    "normalize": True,
    "reduce_labels": False,
    "image_mean": train_mean,
    "image_std": train_std,
    "rescale": False
}

print("\n📊 Image Stats:", SegFormer_config_no_resize["name"])
print("Image mean:", SegFormer_config_no_resize["image_mean"])
print("Image std:", SegFormer_config_no_resize["image_std"])

# 선언된 Config 설정 모음집
experiment_configs = {
    "no_resize_128": SegFormer_config_no_resize
}

# ==================================================
# 사용할 Config 선택
# ==================================================

# 설정 모음집 중 사용할 config 선택
current = experiment_configs["no_resize_128"]

# 선택된 설정을 SegformerImageProcessor에 적용
image_processor = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

print("\n📊 Preprocessing Stats: image_processor")
print("Image mean:", image_processor.image_mean)
print("Image std:", image_processor.image_std)

"""## HuggingFace SegFormer Dataset 객체 정의 및 Dataset 구성"""

# ==================================================
# Dataset 객체 정의 (HuggingFace SegFormer Pretrained)
# ==================================================

# Dataset 객체 정의
class HFSegformerDataset(Dataset):
    def __init__(self, images, masks, processor, debug=False):
        """
        images: np.ndarray  (N, H, W)  또는 (N, H, W, 1)
        masks : np.ndarray  (N, H, W)  또는 (N, H, W, 1)
        """
        self.images = images
        self.masks  = masks
        self.processor = processor
        self.debug = debug
        if self.debug:
            self.mask_stats = []

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        import torch                       # 로컬 import (NameError 방지)

        # 이미지·마스크 추출
        img = self.images[idx].squeeze(-1)         # (H, W)  float32 (0~1)
        msk = self.masks[idx].squeeze(-1).astype(np.int64)  # (H, W) 0/1

        # 흑백 → RGB 3채널 복제
        img_rgb = np.repeat(img[..., None], 3, axis=-1)     # (H,W,3)

        # HuggingFace processor 적용
        inputs = self.processor(
            images            = img_rgb,
            segmentation_maps = msk,
            return_tensors    = "pt"
        )

        # labels 값이 processor 내부에서 int64 타입으로 바뀌므로 Clamp 생략 가능
        # 필요하다면 .clamp(0, 1) 유지
        inputs["labels"] = inputs["labels"].clamp(0, 1)

        # squeeze(0) 해서 (3,H,W) / (H,W) 반환
        out = {k: v.squeeze(0) for k, v in inputs.items()}

        # 디버깅용 통계
        if self.debug:
            self.mask_stats.append(torch.unique(out["labels"]).tolist())

        return out

    # 에포크 끝나고 호출
    def report_mask_stats(self):
        if self.debug and self.mask_stats:
            uniq = sorted({val for sub in self.mask_stats for val in sub})
            print(f"[Epoch Summary] Unique label values: {uniq}")
            self.mask_stats.clear()

# ==================================================
# 데이터셋 구성 함수
# ==================================================

# 데이터셋 구성 -> 학습 수행 함수에서 호출
def build_covid_dataloaders(X_train, y_train,
                             X_val, y_val,
                             X_test, y_test,
                             processor,
                             batch_size=8,
                             debug=False,
                             num_workers=4):

    # Dataset 객체 생성
    train_dataset = HFSegformerDataset(X_train, y_train, processor, debug=debug)
    val_dataset   = HFSegformerDataset(X_val,   y_val,   processor, debug=debug)
    test_dataset  = HFSegformerDataset(X_test,  y_test,  processor, debug=debug)

    # DataLoader 구성
    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                              shuffle=True, num_workers=num_workers)

    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                            shuffle=False, num_workers=num_workers)

    test_loader = DataLoader(test_dataset, batch_size=batch_size,
                             shuffle=False, num_workers=num_workers)

    return train_loader, val_loader, test_loader

"""## HuggingFace SegFormer 모델 생성 및 학습 수행 함수 (데이터셋 구성 및 모델저장 포함)"""

# ==================================================
# 업샘플링을 위한 클래스
# ==================================================

class SegFormerWithUpsample(nn.Module):
    def __init__(self, base_model, upsample_size=(128, 128)):
        super().__init__()
        self.base_model = base_model
        self.upsample = nn.Upsample(size=upsample_size, mode="bilinear", align_corners=False)

    def forward(self, pixel_values, labels=None):
        outputs = self.base_model(pixel_values=pixel_values, labels=labels)
        logits = outputs.logits                    # (B, C, H_out, W_out)
        logits_up = self.upsample(logits)          # (B, C, H, W)

        # 손실 계산을 위해 GT와 크기 일치
        loss = None
        if labels is not None:
            loss = F.cross_entropy(logits_up, labels, ignore_index=255)

        return type('Output', (), {
            'logits': logits_up,
            'loss': loss
        })()

# ==================================================
# Segformer-B0 기반 scratch 모델 생성
# ==================================================

# 전처리기 current 설정(dict)을 기반으로 SegformerConfig를 생성
def create_config_from_preprocessor(
    current,
    num_labels=2,
    hidden_sizes=[32, 64, 160, 256],
    decoder_hidden_size=256,
    dropout=0.1,
    ignore_index=255
):
    config = SegformerConfig(
        num_labels=num_labels,
        image_size=(current["size"]["height"], current["size"]["width"]),
        do_reduce_labels=current.get("reduce_labels", False),
        hidden_sizes=hidden_sizes,
        decoder_hidden_size=decoder_hidden_size,
        classifier_dropout_prob=dropout,
        semantic_loss_ignore_index=ignore_index
    )
    return config

# current 변수에 저장된 전처리기 config를 반영하여 모델 생성
current = experiment_configs["no_resize_128"]
config = create_config_from_preprocessor(current, num_labels=2)
base_model = SegformerForSemanticSegmentation(config)
model = SegFormerWithUpsample(base_model, upsample_size=(128, 128))

# ==================================================
# 학습 수행 함수에 필요한 보조 함수 정의
# ==================================================

# 하나의 epoch 동안 모델을 학습 -> 학습 수행 함수에서 호출
def train_one_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0.0
    total_dice = 0.0

    progress_bar = tqdm(dataloader, desc="🟢 Training", leave=True, dynamic_ncols=True, mininterval=0.1)

    for batch in progress_bar:
        inputs = batch["pixel_values"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(pixel_values=inputs, labels=labels)
        loss = outputs.loss

        with torch.no_grad():
            preds = torch.argmax(outputs.logits, dim=1)
            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()
            dice = compute_dice_score(preds, labels)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        total_dice += dice.item()

    progress_bar.set_description("🟢 Training (completed)")
    return total_loss / len(dataloader), total_dice / len(dataloader)

# 검증 데이터셋에서 성능을 평가 -> 학습 수행 함수에서 호출
def validate(model, dataloader, device):
    model.eval()
    total_loss = 0.0
    total_dice = 0.0
    total_iou = 0.0
    total_precision = 0.0
    total_recall = 0.0
    n = 0

    progress_bar = tqdm(dataloader, desc="🔵 Validating", leave=True, dynamic_ncols=True, mininterval=0.1)

    with torch.no_grad():
        for batch in progress_bar:
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs, labels=labels)
            loss = outputs.loss
            preds = torch.argmax(outputs.logits, dim=1)

            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()
            dice = compute_dice_score(preds, labels)

            y_pred = preds.cpu().numpy().flatten()
            y_true = labels.cpu().numpy().flatten()

            iou = jaccard_score(y_true, y_pred, average="binary", zero_division=0)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)

            total_loss += loss.item()
            total_dice += dice.item()
            total_iou += iou
            total_precision += precision
            total_recall += recall
            n += 1

    progress_bar.set_description("🔵 Validating (completed)")
    return {
        "loss": total_loss / n,
        "dice": total_dice / n,
        "iou": total_iou / n,
        "precision": total_precision / n,
        "recall": total_recall / n
    }

# 이진 Dice Score 계산 -> 학습 수행 함수에서 호출
def compute_dice_score(preds, targets, smooth=1e-6):
    preds = preds.view(-1).float()
    targets = targets.view(-1).float()

    intersection = (preds * targets).sum()
    dice = (2.0 * intersection + smooth) / (preds.sum() + targets.sum() + smooth)

    return dice

# ==================================================
# 학습이 완료된 모델 저장
# ==================================================

# 학습 정상 완료 후 메타데이터와 함께 모델저장 -> 학습 수행 함수에서 호출
def save_segformer_model_simple(
    model,
    processor=None,
    history=None,
    config=None,
    save_dir=None,
    tag=None
):
    """
    SegFormer-B0 모델과 메타데이터를 간단히 저장합니다.

    Args:
        model: 학습 완료된 모델
        processor: SegformerImageProcessor 객체 (선택)
        history: 학습 기록 (loss/dice 등 포함된 dict, 선택)
        config: 학습 설정 정보 (예: batch_size, lr 등, 선택)
        save_dir: 저장 디렉토리명 접두사
        tag: 전처리기 설정명 등 (ex. "no_resize")

    Returns:
        str: 모델 저장 디렉토리 경로
    """
    current_tag = tag or "unknown"
    save_dir = save_dir or f"{current_tag}"

    KST = timezone(timedelta(hours=9))
    timestamp = datetime.now(KST).strftime("%m%d_%H%M")
    model_path = f"/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/{save_dir}_{timestamp}"
    os.makedirs(model_path, exist_ok=True)

    # 모델 저장 (HuggingFace 포맷)
    model.save_pretrained(model_path)
    if processor:
        processor.save_pretrained(model_path)

    # 메타데이터 구성
    def safe_get(metric_name):
        return history[metric_name][-1] if history and metric_name in history else None

    metadata = {
        "timestamp": timestamp,
        "arch": "SegFormer-B0",
        "num_labels": model.config.num_labels,
        "input_size": processor.size if processor and hasattr(processor, "size") else None,
        "label_mode": "binary",
        "mask_mode": "L",
        "is_scratch": True,
        "config": config,
        "metrics": {
            "val_loss": safe_get("val_loss"),
            "val_dice": safe_get("val_dice"),
            "val_iou": safe_get("val_iou"),
            "val_precision": safe_get("val_precision"),
            "val_recall": safe_get("val_recall")
        }
    }

    # 메타데이터 저장
    metadata_filename = f"metadata_{current_tag}.pt"
    torch.save(metadata, os.path.join(model_path, metadata_filename))

    # best weight 같이 저장 (state_dict 형식)
    torch.save(model.state_dict(), os.path.join(model_path, "best_state_dict.pt"))

    print(f"\n✅ Model saved to: {model_path}")
    if processor:
        print("🧪 Processor saved")
    print(f"🧾 Metadata saved as {metadata_filename}")
    return model_path

# ==================================================
# 학습 수행 함수 정의
# ==================================================

# 전처리기 설정 기반으로 생성한 학습 모델 사용
# train_one_epoch, validate, compute_dice_score  함수를 호출하여 학습을 수행

def train_segformer_model(
    model,
    image_processor,
    train_loader,
    val_loader,
    batch_size=4,
    lr=5e-5,
    num_epochs=20,
    weight_decay=1e-4,
    early_stopping_metric="dice",  # "loss" 또는 "dice"
    patience=5,
    tag=None
):
    """
    SegFormer 모델 학습 루프 수행 + EarlyStopping + 모델 저장 포함

    Args:
        model: HuggingFace SegformerForSemanticSegmentation 모델
        image_processor: SegformerImageProcessor
        train_loader: 학습용 DataLoader
        val_loader: 검증용 DataLoader
        batch_size: 학습에 사용할 배치 사이즈
        lr: 학습률 (learning rate)
        num_epochs: 최대 에폭 수
        weight_decay: AdamW 옵티마이저 weight decay
        early_stopping_metric: "dice" or "loss"
        patience: early stopping 허용 에폭 수
        tag: 모델 저장 이름 접두어 (예: "no_resize")
    """

    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    history = {
        "train_loss": [],
        "val_loss": [],
        "train_dice": [],
        "val_dice": [],
        "val_iou": [],
        "val_precision": [],
        "val_recall": []
    }

    best_metric = -np.inf if early_stopping_metric == "dice" else np.inf
    no_improve_epochs = 0

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    if device.type == "cuda":
        print(f"🚀 Training started on device: {device} ({torch.cuda.get_device_name(device)})")
    else:
        print(f"🚀 Training started on device: {device} (CPU only)")

    for epoch in range(1, num_epochs + 1):
        print(f"\n===== Epoch {epoch}/{num_epochs} =====")

        # 🔁 Train
        avg_train_loss, avg_train_dice = train_one_epoch(model, train_loader, optimizer, device)

        # 🔍 Validate
        val_metrics = validate(model, val_loader, device)

        # ✅ 기록
        history["train_loss"].append(avg_train_loss)
        history["train_dice"].append(avg_train_dice)
        history["val_loss"].append(val_metrics["loss"])
        history["val_dice"].append(val_metrics["dice"])
        history["val_iou"].append(val_metrics["iou"])
        history["val_precision"].append(val_metrics["precision"])
        history["val_recall"].append(val_metrics["recall"])

        print(f"📊 Train Loss: {avg_train_loss:.4f}, Dice: {avg_train_dice:.4f}")
        print(f"🧪 Val   Loss: {val_metrics['loss']:.4f}, Dice: {val_metrics['dice']:.4f}")

        # ⏳ Early stopping check
        current_metric = val_metrics["dice"] if early_stopping_metric == "dice" else val_metrics["loss"]
        is_improved = (current_metric > best_metric) if early_stopping_metric == "dice" else (current_metric < best_metric)

        if is_improved:
            best_metric = current_metric
            no_improve_epochs = 0
            print(f"✅ Improved {early_stopping_metric}! Model saved.")
        else:
            no_improve_epochs += 1
            print(f"⚠️ No improvement in {early_stopping_metric} ({no_improve_epochs}/{patience})")

            if no_improve_epochs >= patience:
                print(f"🛑 Early stopping triggered. Best {early_stopping_metric}: {best_metric:.4f}")
                break

    print("\n🏁 Training complete.")

    # 💾 학습 완료 후 모델 저장
    save_segformer_model_simple(
        model=model,
        processor=image_processor,
        history=history,
        config={
            "batch_size": batch_size,
            "lr": lr,
            "num_epochs": num_epochs,
            "weight_decay": weight_decay,
            "early_stopping_metric": early_stopping_metric,
            "patience": patience
        },
        tag=tag or "segformer"
    )

    return model, history

"""## HuggingFace SegFormer 모델평가, 성능지표 및 시각화 자료 출력 함수"""

# ==================================================
# 테스트셋으로 모델 평가 함수 정의
# ==================================================

def evaluate_on_testset(
    model,
    test_loader,          # ← DataLoader 직접 전달
    device=None,
    tag=None,
    return_y=False
):

    device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    total_loss = 0.0
    all_preds, all_labels = [], []

    progress = tqdm(test_loader, desc="🧪 Evaluating on Test Set",
                    leave=True, dynamic_ncols=True, mininterval=0.1)

    with torch.no_grad():
        for batch in progress:
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs, labels=labels)
            total_loss += outputs.loss.item()

            preds = torch.argmax(outputs.logits, dim=1)

            all_preds.append(preds.cpu().numpy().flatten())
            all_labels.append(labels.cpu().numpy().flatten())

    progress.set_description("🧪 Evaluating (completed)")

    y_pred = np.concatenate(all_preds)
    y_true = np.concatenate(all_labels)

    # 지표 계산
    dice      = compute_dice_score(torch.tensor(y_pred),
                                   torch.tensor(y_true)).item()
    iou       = jaccard_score(y_true, y_pred, average="binary")
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall    = recall_score(y_true, y_pred, zero_division=0)
    accuracy  = accuracy_score(y_true, y_pred)
    avg_loss  = total_loss / len(test_loader)

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()
    specificity = tn / (tn + fp + 1e-8)

    print(f"\n✅ [Test Set Evaluation Complete: {tag}]")
    print(f"📊 Loss     : {avg_loss:.4f}")
    print(f"🎯 Dice     : {dice:.4f}")
    print(f"📐 IoU      : {iou:.4f}")
    print(f"📈 Precision: {precision:.4f}")
    print(f"📉 Recall   : {recall:.4f}")
    print(f"✔️ Accuracy : {accuracy:.4f}")
    print(f"🧪 Specificity: {specificity:.4f}")

    result = {
        "loss": avg_loss,
        "dice": dice,
        "iou": iou,
        "precision": precision,
        "recall": recall,
        "accuracy": accuracy,
        "specificity": specificity
    }
    return (result, y_true, y_pred) if return_y else result

# ==================================================
# 시각화 함수: 학습 진행에 따른 Loss, Dice 변화
# ==================================================

def plot_training_history(history, tag=None):
    if not history or "train_loss" not in history:
        print("⚠️ No training history to plot.")
        return

    epochs = range(1, len(history["train_loss"]) + 1)

    plt.figure(figsize=(12, 5))

    # Loss
    if "train_loss" in history and "val_loss" in history:
        plt.subplot(1, 2, 1)
        plt.plot(epochs, history["train_loss"], label="Train Loss")
        plt.plot(epochs, history["val_loss"], label="Val Loss")
        plt.title("Loss per Epoch")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    # Dice
    if "train_dice" in history and "val_dice" in history:
        plt.subplot(1, 2, 2)
        plt.plot(epochs, history["train_dice"], label="Train Dice")
        plt.plot(epochs, history["val_dice"], label="Val Dice")
        plt.title("Dice Score per Epoch")
        plt.xlabel("Epoch")
        plt.ylabel("Dice Score")
        plt.legend()
        plt.grid(True)

    # 제목
    overall_title = "Training History"
    if tag:
        overall_title += f" — {tag}"
    plt.suptitle(overall_title)

    plt.tight_layout()
    plt.show()

# ==================================================
# 시각화 함수: 혼동행렬
# ==================================================

def plot_confusion_matrix(
    y_true,
    y_pred,
    labels=["background (0)", "lesion (1)"],
    normalize=None
):
    """
    혼동행렬을 시각화합니다. normalize 옵션을 통해 정규화도 가능.

    Args:
        y_true (array-like): 정답 레이블
        y_pred (array-like): 예측 레이블
        labels (list): 클래스 라벨 목록
        normalize (str or None): 'true', 'pred', 'all', 또는 None 가능
    """
    valid_normalize = [None, "true", "pred", "all"]
    if normalize not in valid_normalize:
        print(f"⚠️ [Invalid normalize option] '{normalize}' → None으로 대체됩니다.")
        normalize = None
    else:
        print(f"🧮 Confusion matrix normalization: {normalize}")

    cm = confusion_matrix(y_true, y_pred, normalize=normalize)
    print(f"\n📊 Confusion Matrix (normalize={normalize}):\n{cm}")

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f")
    plt.title("Confusion Matrix")
    plt.show()

# ==================================================
# 시각화 함수: 5개 샘플에서 segmentation 결과 비교
# ==================================================

def visualize_random_samples_arrays(model, processor, images_np, masks_np, num_samples=5):
    """
    numpy 배열로 로드된 (H,W) 또는 (H,W,1) 이미지를 시각화.
    images_np: (N,H,W) or (N,H,W,1) float32 0~1
    masks_np : (N,H,W) {0,1}
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device); model.eval()

    idxs = random.sample(range(len(images_np)), num_samples)
    fig, axes = plt.subplots(num_samples, 3, figsize=(10, 4*num_samples))

    if num_samples == 1:
        axes = [axes]

    for row, idx in enumerate(idxs):
        img  = images_np[idx].squeeze()          # (H,W)
        msk  = masks_np[idx].squeeze()           # (H,W)

        # 흑백 → RGB   (0~1 → 0~255 uint8)
        img_rgb = (np.repeat(img[..., None], 3, axis=-1) * 255).astype(np.uint8)
        pil_img = Image.fromarray(img_rgb)

        inputs = processor(images=pil_img, return_tensors="pt").to(device)
        with torch.no_grad():
            pred = model(**inputs).logits.argmax(dim=1).squeeze().cpu().numpy()

        axes[row][0].imshow(pil_img);    axes[row][0].set_title("Original")
        axes[row][1].imshow(msk,  cmap="gray"); axes[row][1].set_title("Ground-Truth")
        axes[row][2].imshow(pred, cmap="gray"); axes[row][2].set_title("Predicted")

        for ax in axes[row]:
            ax.axis("off")

    plt.tight_layout(); plt.show()

# ==================================================
# test set의 모든 image에 대해 dice와 loss를 추출
# ==================================================

def export_per_sample_metrics(model, test_loader, save_csv_path=None):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    dice_list = []
    iou_list = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="🔍 Evaluating per sample"):
            inputs = batch["pixel_values"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(pixel_values=inputs)
            preds = torch.argmax(outputs.logits, dim=1)

            # 해상도 맞추기
            if preds.shape != labels.shape:
                labels = F.interpolate(
                    labels.unsqueeze(1).float(), size=preds.shape[-2:], mode="nearest"
                ).squeeze(1).long()

            for i in range(labels.shape[0]):
                y_true = labels[i].cpu().numpy().flatten()
                y_pred = preds[i].cpu().numpy().flatten()

                dice = f1_score(y_true, y_pred, zero_division=0)
                iou = jaccard_score(y_true, y_pred, zero_division=0)

                dice_list.append(dice)
                iou_list.append(iou)

    df = pd.DataFrame({
        "Dice": dice_list,
        "IoU": iou_list
    })

    if save_csv_path:
        df.to_csv(save_csv_path, index=False)
        print(f"✅ CSV saved: {save_csv_path}")
    else:
        print("⚠️ CSV path not provided — DataFrame not saved.")

    return df

"""## HuggingFace SegFormer 학습 수행 및 테스트"""

# ==================================================
# 학습 수행
# ==================================================

# 학습 수행 직전 전처리기 관련 중요 전역변수 재선언
current = experiment_configs["no_resize_128"]

# 선택된 설정을 SegformerImageProcessor에 적용
image_processor = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

# 학습 수행 직전 모델 수행 관련 중요 전역변수 재선언
config = create_config_from_preprocessor(current, num_labels=2)
model = SegformerForSemanticSegmentation(config)

# 학습 수행 직전 데이터셋 관련 중요 전역변수 재선언
train_loader, val_loader, _ = build_covid_dataloaders(
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    X_test=X_test,   # 무시됨
    y_test=y_test,   # 무시됨
    processor=image_processor,
    batch_size=8,
    num_workers=4,
    debug=True
)

# 조작 가능한 학습설정
train_config = {
    "batch_size": 8,
    "lr": 5e-5,
    "num_epochs": 20,
    "weight_decay": 3e-4,
    "early_stopping_metric": "loss",
    "patience": 7
}

# 위의 학습설정 기반으로 학습수행 함수 호출
model, history = train_segformer_model(
    model=model,
    image_processor=image_processor,
    train_loader=train_loader,
    val_loader=val_loader,
    tag=current["name"],
    **train_config
)

# ==================================================
# 저장된 모델로 테스트 후 성능지표 확인 및 시각화
# ==================================================

# 테스트할 모델이 저장된 경로
saved_model_path = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/SegFormer-b0-no-resize-128_0624_1112"

# 모델 및 전처리기 로드
base_model = SegformerForSemanticSegmentation.from_pretrained(saved_model_path)
model_loaded = SegFormerWithUpsample(base_model, upsample_size=(128, 128))
model_loaded.eval()

# 3) 전처리기 복원 (학습 당시 config에서 재생성)
current = experiment_configs["no_resize_128"]
image_processor_loaded = SegformerImageProcessor(
    do_resize=current["resize"],
    size=current["size"],
    do_normalize=current["normalize"],
    reduce_labels=current["reduce_labels"],
    image_mean=current["image_mean"],
    image_std=current["image_std"],
    do_rescale=current["rescale"]
)

# 모델 식별용 태그 추출
extracted_tag = os.path.basename(saved_model_path)

# 테스트용 DataLoader 구성
_, _, test_loader = build_covid_dataloaders(
    X_train, y_train,
    X_val,   y_val,
    X_test,  y_test,
    processor=image_processor_loaded,
    batch_size=8,
    debug=False,
    num_workers=4
)

# 모델 평가 수행
test_metrics, y_true, y_pred = evaluate_on_testset(
    model=model_loaded,
    test_loader=test_loader,
    tag=extracted_tag,
    return_y=True
)

# 시각화 함수 호출
plot_training_history(history, tag=extracted_tag)
plot_confusion_matrix(y_true, y_pred)
visualize_random_samples_arrays(
    model_loaded,
    image_processor_loaded,
    X_test,
    y_test,
    num_samples=5
)

# dice, loss 추출 함수 호출
df_sample = export_per_sample_metrics(
    model       = model_loaded,
    test_loader = test_loader,
    save_csv_path = "/content/drive/MyDrive/Colab Notebooks/projects/kdt_whole_lung_seg/models/segformer/chest_x_ray/SegFormer-b0-no-resize-128_0624_1112/per_sample_metrics.csv"
)